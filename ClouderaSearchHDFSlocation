/*
Program for Cloudera Search application where User can input a keyword and Output will result no of times the keyword found and
HDFS location of that file. This program is done in Cloudera Quickstart 4.7.0 and Eclipse Juno
Below is the JAVA code and POM.xml file.
*/

POM.xml

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com</groupId>
  <artifactId>solr</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <packaging>jar</packaging>
  <name>solr</name>
  <url>http://maven.apache.org</url>
  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
  </properties>
<repositories>
    <repository> 
      <id>cloudera</id>
      <url>https://repository.cloudera.com/artifactory/cloudera-repos/</url>
    </repository>
  </repositories>
  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-common</artifactId>
      <version>2.0.0-cdh4.7.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-hdfs</artifactId>
      <version>2.0.0-cdh4.7.0</version>
    </dependency> 
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-core</artifactId>
      <version>2.0.0-mr1-cdh4.7.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-mapreduce-client-core</artifactId>
      <version>2.0.0-cdh4.7.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-annotations</artifactId>
      <version>2.0.0-cdh4.7.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.flume.flume-ng-sinks</groupId>
      <artifactId>flume-ng-morphline-solr-sink</artifactId>
      <version>1.4.0-cdh4.7.0</version>
    </dependency>
<dependency>
	<groupId>org.apache.solr</groupId>
	<artifactId>solr-solrj</artifactId>
	<version>3.6.0</version>
</dependency>
<dependency>
	<groupId>org.apache.solr</groupId>
	<artifactId>solr-core</artifactId>
	<version>1.3.0</version>
</dependency>
<dependency>
	<groupId>org.apache.lucene</groupId>
	<artifactId>lucene-queryparser</artifactId>
	<version>4.7.0</version>
</dependency>
<dependency>
	<groupId>org.apache.lucene</groupId>
	<artifactId>lucene-analyzers-common</artifactId>
	<version>4.7.0</version>
</dependency>
<dependency>
	<groupId>org.apache.lucene</groupId>
	<artifactId>lucene-core</artifactId>
	<version>4.7.0</version>
</dependency>
</dependencies>
</project>


JAVA CODE : 

App.java

package com.solr;

import org.apache.hadoop.fs.*;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.util.*;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Reader;
import java.util.Iterator;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.index.CorruptIndexException;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.queryParser.ParseException;
import org.apache.lucene.queryParser.QueryParser;
import org.apache.lucene.search.Hit;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.store.LockObtainFailedException;

public class App 
{
	public static final String FILES_TO_INDEX_DIRECTORY = "hdfs://127.0.0.1:8020/user/cloudera/raw_data/";
	public static final String INDEX_DIRECTORY = "hdfs://127.0.0.1:8020/user/cloudera/raw_index/";
	public static final String FIELD_PATH = "path";
	public static final String FIELD_CONTENTS = "contents"; 
	public static void main(String[] args) throws Exception {

	createIndex();  
	searchIndex("Query");   
	searchIndex("Love");
	System.out.println(" ");
	}
	public static void createIndex() throws CorruptIndexException, LockObtainFailedException, IOException
	{
			Analyzer analyzer = new StandardAnalyzer();
			boolean recreateIndexIfExists = true;
			Configuration config = new Configuration();
			config.set("fs.default.name","hdfs://127.0.0.1:8020/");
			FileSystem dfs = FileSystem.get(config);
			IndexWriter indexWriter = new IndexWriter(INDEX_DIRECTORY, analyzer, recreateIndexIfExists);
			Path pt = new Path(FILES_TO_INDEX_DIRECTORY);
			System.out.println(pt);
			System.out.println(dfs.getChildFileSystems());
			FileStatus[] status = dfs.listStatus(new Path("hdfs://127.0.0.1:8020/user/cloudera/raw_data/"));
			System.out.println("Files are "+status.length);
			for(int i=0; i<status.length;i++) {
			Document document = new Document();
			String path = dfs.getCanonicalServiceName();
			System.out.println(status[i]);
			System.out.println("\n\npath is "+path+"\n\n");
			Path pt1 = new Path(status[i].getPath().toString());
			document.add(new Field(FIELD_PATH, status[i].getPath().toString(), Field.Store.YES, Field.Index.UN_TOKENIZED));
			System.out.println("before br");
			BufferedReader br = new BufferedReader(new InputStreamReader(dfs.open(pt1)));
			System.out.println("read line"+br.readLine());
			//Reader reader = new FileReader(dfs);
			document.add(new Field(FIELD_CONTENTS, br));
			indexWriter.addDocument(document);
			}
			indexWriter.optimize();
			indexWriter.close();
	}
	public static void searchIndex(String searchString) throws IOException, ParseException 
	{
		System.out.println("Searching for '" + searchString + "'");
		Directory directory = FSDirectory.getDirectory(INDEX_DIRECTORY);
		IndexReader indexReader = IndexReader.open(directory);
		IndexSearcher indexSearcher = new IndexSearcher(indexReader);
		Analyzer analyzer = new StandardAnalyzer();
		QueryParser queryParser = new QueryParser(FIELD_CONTENTS, analyzer);
		Query query = queryParser.parse(searchString);
		Hits hits = indexSearcher.search(query);
		System.out.println("Number of hits: " + hits.length());
			Iterator<Hit> it = hits.iterator();
			while (it.hasNext()) {
				Hit hit = it.next();
				Document document = hit.getDocument();
				String path = document.get(FIELD_PATH);
				System.out.println("Hit: " + path);
				System.out.println("roate");
			}
	}
}





